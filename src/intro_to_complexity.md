# 複雜度的概念

複雜度是一個我們常常拿來衡量一個演算法「快不快」的工具。假如你寫過程式，那你大概可以猜想到，一個程式執行愈多行的程式碼，所花的時間就愈多，像是一個跑 100 次的迴圈就比跑 10 次的迴圈要多花大概 10 倍的時間。當然每個指令所花的時間也不會一樣，像是加法可能就會算的比乘法快一些，不過我們其實關心的不是這些零頭，而是總體而言所需的時間跟「問題的大小」之間的關係。

什麼是「問題的大小」呢？比如說，把 100 個數字從小排到大這個問題，就比把 10 個數字從小排到大這個問題大 10 倍。

假如我們把要排序的數字個數當成 $$N$$ ，那麼我們可以假設某個 $$T(N) = $$ 排序這 $$N$$ 個數字所需的時間。

我們可以想像 $$T(n)$$ 可能等於 $$3.4341 \cdot 10^{-5} n^2 + 5.463 \cdot 10^{-6} n + 4.2461 \cdot 10^{-6}$$ 這類的東西，也可能不是。但總之，我們關心的不是那些常數，而是 $$T(n)$$ 對於 $$n$$ 成長的「趨勢」。可以看出來在上面那個 $$T(n)$$ 中，當 $$n$$ 非常大時，$$n^2$$ 會是最重要的一項。於是，我們說$$T(n) = O(n^2)$$。其中 $$f(n) = O(g(n))$$ 的意思，在概念上是說 $$f(n)$$ 最多不會成長得比 $$g(n)$$ 快，更嚴謹的數學定義是

$$f(n) = O(n) \rightarrow \text{存在常數} n, c \text{使得} f(n) \leq c \cdot g(n) \forall n \geq N$$

例如：$$N^2 + 100000000000000N = O(N^2)$$，因為當 $$N \geq 1$$ 時，$$N^2 + 100000000000000N \leq N^2 + 100000000000000N = 100000000000001N$$。


